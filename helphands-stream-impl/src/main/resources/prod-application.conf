play.modules.enabled += org.example.stream.impl.StreamModule

lagom.persistence.ask-timeout = 10s

stream.cassandra.keyspace = stream

cassandra-journal.keyspace = ${stream.cassandra.keyspace}
cassandra-snapshot-store.keyspace = ${stream.cassandra.keyspace}
lagom.persistence.read-side.cassandra.keyspace = ${stream.cassandra.keyspace}

cassandra.default {
  ## list the contact points  here
  contact-points = ["10.96.0.100"]
  ## override Lagomâ€™s ServiceLocator-based ConfigSessionProvider
  session-provider = akka.persistence.cassandra.ConfigSessionProvider
}

cassandra-journal {
  contact-points = ${cassandra.default.contact-points}
  session-provider = ${cassandra.default.session-provider}
}

cassandra-snapshot-store {
  contact-points = ${cassandra.default.contact-points}
  session-provider = ${cassandra.default.session-provider}
}

lagom.persistence.read-side.cassandra {
  contact-points = ${cassandra.default.contact-points}
  session-provider = ${cassandra.default.session-provider}
}

play {
    server {
        pidfile.path = "/dev/null"
    }

    http.secret.key = "${APPLICATION_SECRET}"
}

# akka {
#     discovery.method = akka-dns
#
#     cluster {
#         shutdown-after-unsuccessful-join-seed-nodes = 60s
#     }
#
#     management {
#         cluster.bootstrap {
#             contact-point-discovery {
#                 discovery-method = kubernetes-api
#                 service-name = "hello"
#                 required-contact-point-nr = ${REQUIRED_CONTACT_POINT_NR}
#             }
#         }
#     }
# }


lagom.akka.discovery {
  # When the service lookup regex fails, the defaults are used for the port and protocol.
  defaults {

    # The default port name. Blank if no port name should be added by default.
    port-name = http

    # The default port protocol. Blank if no port protocol should be added by default.
    port-protocol = tcp

    # The default scheme to use in returned URIs if not defined in the port-name-scheme-mappings.
    scheme = http
  }


  service-name-mappings {
    hello-stream {
      # lookup is done using 'my-service-name'
      # but translated to SRV string _http._tcp.default.svc.cluster.local
      lookup = hello.default.svc.cluster.local
      port-name = http
      port-protocol = tcp
      scheme = http
      # lookup = _http._tcp.default.svc.cluster.local
    }

    # lookup is done using 'my-cassandra-server'
    # but translated to cassandra.default.svc.cluster.local
    # and without port name and protocol, ie: DNS A lookup
    cassandra-0 {
      lookup = cassandra-0.default.svc.cluster.local
      port-name = null
      port-protocol = null
    }
  }

  # The timeout for a successful lookup.
  lookup-timeout = 5 seconds
}

lagom.broker.kafka {
  # The name of the Kafka service to look up out of the service locator.
  # If this is an empty string, then a service locator lookup will not be done,
  # and the brokers configuration will be used instead.
  service-name = ${KAFKA_SERVICE}

  # The URLs of the Kafka brokers. Separate each URL with a comma.
  # This will be ignored if the service-name configuration is non empty.
  brokers = ${KAFKA_BROKERS}

  # A mapping of Lagom topic id to real Kafka topic name.
  # For example:
  # topic-name-mappings {
  #   topic-id = kafka-topic-name
  # }
  topic-name-mappings {
  }

  client {
    default {
      # how long should we wait when retrieving the last known offset
      offset-timeout = 5s

      # Exponential backoff for failures
      failure-exponential-backoff {
        # minimum (initial) duration until processor is started again
        # after failure
        min = 3s

        # the exponential back-off is capped to this duration
        max = 30s

        # additional random delay is based on this factor
        random-factor = 0.2
      }
    }

    # configuration used by the Lagom Kafka consumer
    consumer {
      offset-timeout = ${lagom.broker.kafka.client.default.offset-timeout}
      failure-exponential-backoff = ${lagom.broker.kafka.client.default.failure-exponential-backoff}

      # The number of offsets that will be buffered to allow the consumer flow to
      # do its own buffering. This should be set to a number that is at least as
      # large as the maximum amount of buffering that the consumer flow will do,
      # if the consumer buffer buffers more than this, the offset buffer will
      # backpressure and cause the stream to stop.
      offset-buffer = 100

      # Number of messages batched together by the consumer before the related messages'
      # offsets are committed to Kafka.
      # By increasing the batching-size you are trading speed with the risk of having
      # to re-process a larger number of messages if a failure occurs.
      # The value provided must be strictly greater than zero.
      batching-size = 20

      # Interval of time waited by the consumer before the currently batched messages'
      # offsets are committed to Kafka.
      # This parameter is useful to ensure that messages' offsets are always committed
      # within a fixed amount of time.
      # The value provided must be strictly greater than zero.
      batching-interval = 5 seconds

      # Parallelsim for async committing to Kafka
      # The value provided must be strictly greater than zero.
      batching-parallelism = 3
    }
  }
}